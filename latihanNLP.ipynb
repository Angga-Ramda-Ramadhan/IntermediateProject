{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "teks = \"Perekonomian Indonesia sedang dalam pertumbuhan yang membanggakan.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Perekonomian',\n",
       " 'Indonesia',\n",
       " 'sedang',\n",
       " 'dalam',\n",
       " 'pertumbuhan',\n",
       " 'yang',\n",
       " 'membanggakan',\n",
       " '.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_kata = word_tokenize(teks)\n",
    "token_kata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_indonesian = set(stopwords.words('indonesian'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Perekonomian', 'Indonesia', 'pertumbuhan', 'membanggakan', '.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kata_penting = [kata for kata in token_kata if kata.lower() not in stopwords_indonesian]\n",
    "kata_penting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Perekonomian Indonesia pertumbuhan membanggakan .'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teks_update = ' '.join(kata_penting)\n",
    "teks_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teks asli: Perekonomian Indonesia sedang dalam pertumbuhan yang membanggakan.\n",
      "Teks setelah filtering stopwords NLTK: Perekonomian Indonesia pertumbuhan membanggakan .\n"
     ]
    }
   ],
   "source": [
    "print(\"Teks asli:\", teks)\n",
    "print(\"Teks setelah filtering stopwords NLTK:\", teks_update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "from nltk.tokenize import word_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "factory = StopWordRemoverFactory()\n",
    "stopwords_sastrawi = factory.get_stop_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Perekonomian',\n",
       " 'Indonesia',\n",
       " 'sedang',\n",
       " 'dalam',\n",
       " 'pertumbuhan',\n",
       " 'yang',\n",
       " 'membanggakan',\n",
       " '.']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_kata = word_tokenize(teks)\n",
    "tokens_kata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Perekonomian', 'Indonesia', 'sedang', 'pertumbuhan', 'membanggakan', '.']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kata_penting = [kata for kata in tokens_kata if kata.lower() not in stopwords_sastrawi]\n",
    "kata_penting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Perekonomian Indonesia sedang pertumbuhan membanggakan .'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teks_update = ' '.join(kata_penting)\n",
    "teks_update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [\"running\", \"runs\", \"runner\", \"ran\", \"easily\", \"fairness\", \"better\", \"best\", \"cats\", \"cacti\", \"geese\", \"rocks\", \"oxen\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kata asli running, Kata setelah stem run\n",
      "Kata asli runs, Kata setelah stem run\n",
      "Kata asli runner, Kata setelah stem runner\n",
      "Kata asli ran, Kata setelah stem ran\n",
      "Kata asli easily, Kata setelah stem easili\n",
      "Kata asli fairness, Kata setelah stem fair\n",
      "Kata asli better, Kata setelah stem better\n",
      "Kata asli best, Kata setelah stem best\n",
      "Kata asli cats, Kata setelah stem cat\n",
      "Kata asli cacti, Kata setelah stem cacti\n",
      "Kata asli geese, Kata setelah stem gees\n",
      "Kata asli rocks, Kata setelah stem rock\n",
      "Kata asli oxen, Kata setelah stem oxen\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    stemmed_word = stemmer.stem(word)\n",
    "    print(f\"Kata asli {word}, Kata setelah stem {stemmed_word}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sebelum lemmatize Run, setelah lemmatize run\n",
      "sebelum lemmatize Cat, setelah lemmatize cat\n",
      "sebelum lemmatize Good, setelah lemmatize good\n",
      "sebelum lemmatize Goose, setelah lemmatize goose\n",
      "sebelum lemmatize Rock, setelah lemmatize rock\n",
      "sebelum lemmatize City, setelah lemmatize city\n",
      "sebelum lemmatize Big, setelah lemmatize big\n",
      "sebelum lemmatize Happy, setelah lemmatize happy\n",
      "sebelum lemmatize Run, setelah lemmatize run\n",
      "sebelum lemmatize Sleep, setelah lemmatize sleep\n"
     ]
    }
   ],
   "source": [
    "words = [\"Run\", \"Cat\", \"Good\", \"Goose\", \"Rock\", \"City\", \"Big\", \"Happy\", \"Run\", \"Sleep\"]\n",
    "\n",
    "for word in words:\n",
    "    lemma_word = lemmatizer.lemmatize(word.lower())\n",
    "    print(f'sebelum lemmatize {word}, setelah lemmatize {lemma_word}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extrasi Fitur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = [\n",
    "    'Saya suka makan bakso',\n",
    "    'Bakso enak dan lezat',\n",
    "    'Makanan favorit saya adalah nasi goreng',\n",
    "    'Nasi goreng pedas adalah makanan favorit saya',\n",
    "    'Saya suka makanan manis seperti es krim',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_word = [word_tokenize(sentence.lower()) for sentence in text_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['saya', 'suka', 'makan', 'bakso'],\n",
       " ['bakso', 'enak', 'dan', 'lezat'],\n",
       " ['makanan', 'favorit', 'saya', 'adalah', 'nasi', 'goreng'],\n",
       " ['nasi', 'goreng', 'pedas', 'adalah', 'makanan', 'favorit', 'saya'],\n",
       " ['saya', 'suka', 'makanan', 'manis', 'seperti', 'es', 'krim']]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(sentences=tokenized_word, vector_size=100, window=5, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kata yang mirip dengan 'bakso':  [('manis', 0.2529163062572479), ('nasi', 0.17018672823905945), ('enak', 0.15006466209888458)]\n"
     ]
    }
   ],
   "source": [
    "simmilar_words = word_vectors.most_similar('bakso', topn=3)\n",
    "print(\"kata yang mirip dengan 'bakso': \", simmilar_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    \"Saya suka makan bakso\",\n",
    "    \"Bakso enak dan lezat\",\n",
    "    \"Makanan favorit saya adalah nasi goreng\",\n",
    "    \"Nasi goreng pedas adalah makanan favorit saya\",\n",
    "    \"Saya suka makanan manis seperti es krim\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab:  {'saya': 14, 'suka': 16, 'makan': 9, 'bakso': 1, 'enak': 3, 'dan': 2, 'lezat': 8, 'makanan': 10, 'favorit': 5, 'adalah': 0, 'nasi': 12, 'goreng': 6, 'pedas': 13, 'manis': 11, 'seperti': 15, 'es': 4, 'krim': 7}\n"
     ]
    }
   ],
   "source": [
    "tfidf_vect = TfidfVectorizer()\n",
    "\n",
    "tfodf_matrix = tfidf_vect.fit_transform(documents)\n",
    "\n",
    "print(\"Vocab: \", tfidf_vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Matrix:\n",
      "[[0.         0.49851188 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.61789262 0.         0.\n",
      "  0.         0.         0.34810993 0.         0.49851188]\n",
      " [0.         0.42224214 0.52335825 0.52335825 0.         0.\n",
      "  0.         0.         0.52335825 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.        ]\n",
      " [0.43951606 0.         0.         0.         0.         0.43951606\n",
      "  0.43951606 0.         0.         0.         0.36483803 0.\n",
      "  0.43951606 0.         0.30691325 0.         0.        ]\n",
      " [0.38596041 0.         0.         0.         0.         0.38596041\n",
      "  0.38596041 0.         0.         0.         0.320382   0.\n",
      "  0.38596041 0.47838798 0.26951544 0.         0.        ]\n",
      " [0.         0.         0.         0.         0.42966246 0.\n",
      "  0.         0.42966246 0.         0.         0.28774996 0.42966246\n",
      "  0.         0.         0.24206433 0.42966246 0.34664897]]\n"
     ]
    }
   ],
   "source": [
    "print(\"TF-IDF Matrix:\")\n",
    "print(tfodf_matrix.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BagofWord (BoW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "documents = [\n",
    "    \"Ini adalah contoh dokumen pertama.\",\n",
    "    \"Ini adalah dokumen kedua.\",\n",
    "    \"Ini adalah dokumen ketiga.\",\n",
    "    \"Ini adalah contoh contoh contoh.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_matrix = vectorizer.fit_transform(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 1, 0, 0, 1],\n",
       "       [1, 0, 1, 1, 1, 0, 0],\n",
       "       [1, 0, 1, 1, 0, 1, 0],\n",
       "       [1, 3, 0, 1, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_matrix.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriks BoW:\n",
      "[[1 1 1 1 0 0 1]\n",
      " [1 0 1 1 1 0 0]\n",
      " [1 0 1 1 0 1 0]\n",
      " [1 3 0 1 0 0 0]]\n",
      "\n",
      "Daftar Fitur:\n",
      "['adalah' 'contoh' 'dokumen' 'ini' 'kedua' 'ketiga' 'pertama']\n"
     ]
    }
   ],
   "source": [
    "print(\"Matriks BoW:\")\n",
    "print(bow_matrix.toarray())\n",
    " \n",
    "print(\"\\nDaftar Fitur:\")\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"Saya suka makan bakso enak di warung dekat rumah.\",\n",
    "    \"Nasi goreng adalah salah satu makanan favorit saya.\",\n",
    "    \"Es krim coklat sangat lezat dan menyegarkan.\",\n",
    "    \"Saat hari hujan, saya suka minum teh hangat.\",\n",
    "    \"Pemandangan pegunungan di pagi hari sangat indah.\",\n",
    "    \"Bola basket adalah olahraga favorit saya sejak kecil.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in sentences:\n",
    "    words = sentence.split()\n",
    "    unigrams = list(ngrams(words, 1))\n",
    "    bigrams = list(ngrams(words, 2))\n",
    "    trigrams = list(ngrams(words, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Kalimat: Bola basket adalah olahraga favorit saya sejak kecil.\n",
      "1-gram:\n",
      "('Bola',)\n",
      "('basket',)\n",
      "('adalah',)\n",
      "('olahraga',)\n",
      "('favorit',)\n",
      "('saya',)\n",
      "('sejak',)\n",
      "('kecil.',)\n",
      "\n",
      "2-gram:\n",
      "('Bola', 'basket')\n",
      "('basket', 'adalah')\n",
      "('adalah', 'olahraga')\n",
      "('olahraga', 'favorit')\n",
      "('favorit', 'saya')\n",
      "('saya', 'sejak')\n",
      "('sejak', 'kecil.')\n",
      "\n",
      "3-gram:\n",
      "('Bola', 'basket', 'adalah')\n",
      "('basket', 'adalah', 'olahraga')\n",
      "('adalah', 'olahraga', 'favorit')\n",
      "('olahraga', 'favorit', 'saya')\n",
      "('favorit', 'saya', 'sejak')\n",
      "('saya', 'sejak', 'kecil.')\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nKalimat:\", sentence)\n",
    "print(\"1-gram:\")\n",
    "for gram in unigrams:\n",
    "    print(gram)\n",
    "print(\"\\n2-gram:\")\n",
    "for gram in bigrams:\n",
    "    print(gram)\n",
    "print(\"\\n3-gram:\")\n",
    "for gram in trigrams:\n",
    "    print(gram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Studi Kasus NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from google_play_scraper import app, reviews, Sort, reviews_all\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # Menonaktifkan peringatan chaining\n",
    "import numpy as np\n",
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import datetime as dt\n",
    "import re\n",
    "import string\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "import nltk  # Import pustaka NLTK (Natural Language Toolkit).\n",
    "nltk.download('punkt')  # Mengunduh dataset yang diperlukan untuk tokenisasi teks.\n",
    "nltk.download('stopwords')  # Mengunduh dataset yang berisi daftar kata-kata berhenti (stopwords) dalam berbagai bahasa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrapreview = reviews_all(\n",
    "    'com.byu.id',\n",
    "    lang='id',\n",
    "    country='id',\n",
    "    sort=Sort.MOST_RELEVANT,\n",
    "    count=1000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open('ulasan_aplikasi.csv', mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Review'])\n",
    "    for review in scrapreview:\n",
    "        writer.writerow([review['content']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_review = pd.DataFrame(scrapreview)\n",
    "app_review.shape\n",
    "app_review.head()\n",
    "app_review.to_csv('ulasan_aplikasi.csv', index=False)\n",
    "\n",
    "jumlah_ulasan, jumlah_kolom = app_review.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewId</th>\n",
       "      <th>userName</th>\n",
       "      <th>userImage</th>\n",
       "      <th>content</th>\n",
       "      <th>score</th>\n",
       "      <th>thumbsUpCount</th>\n",
       "      <th>reviewCreatedVersion</th>\n",
       "      <th>at</th>\n",
       "      <th>replyContent</th>\n",
       "      <th>repliedAt</th>\n",
       "      <th>appVersion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>77b1715e-885d-47c7-b958-41c18f60593d</td>\n",
       "      <td>Pengguna Google</td>\n",
       "      <td>https://play-lh.googleusercontent.com/EGemoI2N...</td>\n",
       "      <td>sangat rekomended buat kalian yang mau melatih...</td>\n",
       "      <td>1</td>\n",
       "      <td>361</td>\n",
       "      <td>1.59.0</td>\n",
       "      <td>2024-12-19 23:08:33</td>\n",
       "      <td>Hi, Kak. Maaf banget jadi bikin gak nyaman. Ni...</td>\n",
       "      <td>2024-12-20 07:05:11</td>\n",
       "      <td>1.59.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>98f0f6db-3225-4f54-94cc-0d920622db63</td>\n",
       "      <td>Pengguna Google</td>\n",
       "      <td>https://play-lh.googleusercontent.com/EGemoI2N...</td>\n",
       "      <td>Untuk sekarang saya kasih segini dulu. Kecewa ...</td>\n",
       "      <td>1</td>\n",
       "      <td>96</td>\n",
       "      <td>1.59.0</td>\n",
       "      <td>2024-12-19 20:01:50</td>\n",
       "      <td>Hi, Kak. Maaf banget jadi bikin gak nyaman. Ni...</td>\n",
       "      <td>2024-12-20 07:08:21</td>\n",
       "      <td>1.59.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>427de1b2-99ea-4d93-86cb-acb059c1ef7f</td>\n",
       "      <td>Pengguna Google</td>\n",
       "      <td>https://play-lh.googleusercontent.com/EGemoI2N...</td>\n",
       "      <td>Sayang ya. By.u ini menarik sekali tapi bener ...</td>\n",
       "      <td>3</td>\n",
       "      <td>33</td>\n",
       "      <td>1.59.0</td>\n",
       "      <td>2024-12-26 16:10:50</td>\n",
       "      <td>Hi, Kak. Maaf banget jadi bikin gak nyaman. Ni...</td>\n",
       "      <td>2024-12-26 21:11:56</td>\n",
       "      <td>1.59.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9e145aa3-7c51-402d-b8da-8911061e07da</td>\n",
       "      <td>Pengguna Google</td>\n",
       "      <td>https://play-lh.googleusercontent.com/EGemoI2N...</td>\n",
       "      <td>Cuaca baik baiknya saja dan paket data baru sa...</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1.59.0</td>\n",
       "      <td>2025-01-05 17:22:14</td>\n",
       "      <td>Hi, Kak. Maaf banget jadi bikin gak nyaman. Ni...</td>\n",
       "      <td>2025-01-06 08:36:34</td>\n",
       "      <td>1.59.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b9a8d064-7751-436c-bb53-35fa167a15bb</td>\n",
       "      <td>Pengguna Google</td>\n",
       "      <td>https://play-lh.googleusercontent.com/EGemoI2N...</td>\n",
       "      <td>sy pengguna provider ini dari awal 2020, sedih...</td>\n",
       "      <td>1</td>\n",
       "      <td>49</td>\n",
       "      <td>1.59.0</td>\n",
       "      <td>2025-01-02 02:38:06</td>\n",
       "      <td>Hi, Kak. Maaf banget jadi bikin gak nyaman. Ni...</td>\n",
       "      <td>2025-01-02 07:08:01</td>\n",
       "      <td>1.59.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               reviewId         userName  \\\n",
       "0  77b1715e-885d-47c7-b958-41c18f60593d  Pengguna Google   \n",
       "1  98f0f6db-3225-4f54-94cc-0d920622db63  Pengguna Google   \n",
       "2  427de1b2-99ea-4d93-86cb-acb059c1ef7f  Pengguna Google   \n",
       "3  9e145aa3-7c51-402d-b8da-8911061e07da  Pengguna Google   \n",
       "4  b9a8d064-7751-436c-bb53-35fa167a15bb  Pengguna Google   \n",
       "\n",
       "                                           userImage  \\\n",
       "0  https://play-lh.googleusercontent.com/EGemoI2N...   \n",
       "1  https://play-lh.googleusercontent.com/EGemoI2N...   \n",
       "2  https://play-lh.googleusercontent.com/EGemoI2N...   \n",
       "3  https://play-lh.googleusercontent.com/EGemoI2N...   \n",
       "4  https://play-lh.googleusercontent.com/EGemoI2N...   \n",
       "\n",
       "                                             content  score  thumbsUpCount  \\\n",
       "0  sangat rekomended buat kalian yang mau melatih...      1            361   \n",
       "1  Untuk sekarang saya kasih segini dulu. Kecewa ...      1             96   \n",
       "2  Sayang ya. By.u ini menarik sekali tapi bener ...      3             33   \n",
       "3  Cuaca baik baiknya saja dan paket data baru sa...      3              7   \n",
       "4  sy pengguna provider ini dari awal 2020, sedih...      1             49   \n",
       "\n",
       "  reviewCreatedVersion                  at  \\\n",
       "0               1.59.0 2024-12-19 23:08:33   \n",
       "1               1.59.0 2024-12-19 20:01:50   \n",
       "2               1.59.0 2024-12-26 16:10:50   \n",
       "3               1.59.0 2025-01-05 17:22:14   \n",
       "4               1.59.0 2025-01-02 02:38:06   \n",
       "\n",
       "                                        replyContent           repliedAt  \\\n",
       "0  Hi, Kak. Maaf banget jadi bikin gak nyaman. Ni... 2024-12-20 07:05:11   \n",
       "1  Hi, Kak. Maaf banget jadi bikin gak nyaman. Ni... 2024-12-20 07:08:21   \n",
       "2  Hi, Kak. Maaf banget jadi bikin gak nyaman. Ni... 2024-12-26 21:11:56   \n",
       "3  Hi, Kak. Maaf banget jadi bikin gak nyaman. Ni... 2025-01-06 08:36:34   \n",
       "4  Hi, Kak. Maaf banget jadi bikin gak nyaman. Ni... 2025-01-02 07:08:01   \n",
       "\n",
       "  appVersion  \n",
       "0     1.59.0  \n",
       "1     1.59.0  \n",
       "2     1.59.0  \n",
       "3     1.59.0  \n",
       "4     1.59.0  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app_review.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 117000 entries, 0 to 116999\n",
      "Data columns (total 11 columns):\n",
      " #   Column                Non-Null Count   Dtype         \n",
      "---  ------                --------------   -----         \n",
      " 0   reviewId              117000 non-null  object        \n",
      " 1   userName              117000 non-null  object        \n",
      " 2   userImage             117000 non-null  object        \n",
      " 3   content               116999 non-null  object        \n",
      " 4   score                 117000 non-null  int64         \n",
      " 5   thumbsUpCount         117000 non-null  int64         \n",
      " 6   reviewCreatedVersion  98973 non-null   object        \n",
      " 7   at                    117000 non-null  datetime64[ns]\n",
      " 8   replyContent          108322 non-null  object        \n",
      " 9   repliedAt             108322 non-null  datetime64[ns]\n",
      " 10  appVersion            98973 non-null   object        \n",
      "dtypes: datetime64[ns](2), int64(2), object(7)\n",
      "memory usage: 9.8+ MB\n"
     ]
    }
   ],
   "source": [
    "app_review.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "reviewId                    0\n",
       "userName                    0\n",
       "userImage                   0\n",
       "content                     1\n",
       "score                       0\n",
       "thumbsUpCount               0\n",
       "reviewCreatedVersion    18027\n",
       "at                          0\n",
       "replyContent             8678\n",
       "repliedAt                8678\n",
       "appVersion              18027\n",
       "dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app_review.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "reviewId                0\n",
       "userName                0\n",
       "userImage               0\n",
       "content                 0\n",
       "score                   0\n",
       "thumbsUpCount           0\n",
       "reviewCreatedVersion    0\n",
       "at                      0\n",
       "replyContent            0\n",
       "repliedAt               0\n",
       "appVersion              0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_df = app_review.dropna()\n",
    "clean_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaningText(text):\n",
    "    text = re.sub(r'@[A-Za-z0-9]+', '', text) # menghapus mention\n",
    "    text = re.sub(r'#[A-Za-z0-9]+', '', text) # menghapus hashtag\n",
    "    text = re.sub(r'RT[\\s]', '', text) # menghapus RT\n",
    "    text = re.sub(r\"http\\S+\", '', text) # menghapus link\n",
    "    text = re.sub(r'[0-9]+', '', text) # menghapus angka\n",
    "    text = re.sub(r'[^\\w\\s]', '', text) # menghapus karakter selain huruf dan angka\n",
    " \n",
    "    text = text.replace('\\n', ' ') # mengganti baris baru dengan spasi\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation)) # menghapus semua tanda baca\n",
    "    text = text.strip(' ') # menghapus karakter spasi dari kiri dan kanan teks\n",
    "    return text\n",
    " \n",
    "def casefoldingText(text): # Mengubah semua karakter dalam teks menjadi huruf kecil\n",
    "    text = text.lower()\n",
    "    return text\n",
    " \n",
    "def tokenizingText(text): # Memecah atau membagi string, teks menjadi daftar token\n",
    "    text = word_tokenize(text)\n",
    "    return text\n",
    " \n",
    "def filteringText(text): # Menghapus stopwords dalam teks\n",
    "    listStopwords = set(stopwords.words('indonesian'))\n",
    "    listStopwords1 = set(stopwords.words('english'))\n",
    "    listStopwords.update(listStopwords1)\n",
    "    listStopwords.update(['iya','yaa','gak','nya','na','sih','ku',\"di\",\"ga\",\"ya\",\"gaa\",\"loh\",\"kah\",\"woi\",\"woii\",\"woy\"])\n",
    "    filtered = []\n",
    "    for txt in text:\n",
    "        if txt not in listStopwords:\n",
    "            filtered.append(txt)\n",
    "    text = filtered\n",
    "    return text\n",
    " \n",
    "def stemmingText(text): # Mengurangi kata ke bentuk dasarnya yang menghilangkan imbuhan awalan dan akhiran atau ke akar kata\n",
    "    # Membuat objek stemmer\n",
    "    factory = StemmerFactory()\n",
    "    stemmer = factory.create_stemmer()\n",
    " \n",
    "    # Memecah teks menjadi daftar kata\n",
    "    words = text.split()\n",
    " \n",
    "    # Menerapkan stemming pada setiap kata dalam daftar\n",
    "    stemmed_words = [stemmer.stem(word) for word in words]\n",
    " \n",
    "    # Menggabungkan kata-kata yang telah distem\n",
    "    stemmed_text = ' '.join(stemmed_words)\n",
    " \n",
    "    return stemmed_text\n",
    " \n",
    "def toSentence(list_words): # Mengubah daftar kata menjadi kalimat\n",
    "    sentence = ' '.join(word for word in list_words)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "slangwords = {\"@\": \"di\", \"abis\": \"habis\", \"wtb\": \"beli\", \"masi\": \"masih\", \"wts\": \"jual\", \"wtt\": \"tukar\", \"bgt\": \"banget\", \"maks\": \"maksimal\" }\n",
    "def fix_slangwords(text):\n",
    "    words = text.split()\n",
    "    fixed_words = []\n",
    " \n",
    "    for word in words:\n",
    "        if word.lower() in slangwords:\n",
    "            fixed_words.append(slangwords[word.lower()])\n",
    "        else:\n",
    "            fixed_words.append(word)\n",
    " \n",
    "    fixed_text = ' '.join(fixed_words)\n",
    "    return fixed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Membersihkan teks dan menyimpannya di kolom 'text_clean'\n",
    "clean_df['text_clean'] = clean_df['content'].apply(cleaningText)\n",
    " \n",
    "# Mengubah huruf dalam teks menjadi huruf kecil dan menyimpannya di 'text_casefoldingText'\n",
    "clean_df['text_casefoldingText'] = clean_df['text_clean'].apply(casefoldingText)\n",
    " \n",
    "# Mengganti kata-kata slang dengan kata-kata standar dan menyimpannya di 'text_slangwords'\n",
    "clean_df['text_slangwords'] = clean_df['text_casefoldingText'].apply(fix_slangwords)\n",
    " \n",
    "# Memecah teks menjadi token (kata-kata) dan menyimpannya di 'text_tokenizingText'\n",
    "clean_df['text_tokenizingText'] = clean_df['text_slangwords'].apply(tokenizingText)\n",
    " \n",
    "# Menghapus kata-kata stop (kata-kata umum) dan menyimpannya di 'text_stopword'\n",
    "clean_df['text_stopword'] = clean_df['text_tokenizingText'].apply(filteringText)\n",
    " \n",
    "# Menggabungkan token-token menjadi kalimat dan menyimpannya di 'text_akhir'\n",
    "clean_df['text_akhir'] = clean_df['text_stopword'].apply(toSentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewId</th>\n",
       "      <th>userName</th>\n",
       "      <th>userImage</th>\n",
       "      <th>content</th>\n",
       "      <th>score</th>\n",
       "      <th>thumbsUpCount</th>\n",
       "      <th>reviewCreatedVersion</th>\n",
       "      <th>at</th>\n",
       "      <th>replyContent</th>\n",
       "      <th>repliedAt</th>\n",
       "      <th>appVersion</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>text_casefoldingText</th>\n",
       "      <th>text_slangwords</th>\n",
       "      <th>text_tokenizingText</th>\n",
       "      <th>text_stopword</th>\n",
       "      <th>text_akhir</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>77b1715e-885d-47c7-b958-41c18f60593d</td>\n",
       "      <td>Pengguna Google</td>\n",
       "      <td>https://play-lh.googleusercontent.com/EGemoI2N...</td>\n",
       "      <td>sangat rekomended buat kalian yang mau melatih...</td>\n",
       "      <td>1</td>\n",
       "      <td>361</td>\n",
       "      <td>1.59.0</td>\n",
       "      <td>2024-12-19 23:08:33</td>\n",
       "      <td>Hi, Kak. Maaf banget jadi bikin gak nyaman. Ni...</td>\n",
       "      <td>2024-12-20 07:05:11</td>\n",
       "      <td>1.59.0</td>\n",
       "      <td>sangat rekomended buat kalian yang mau melatih...</td>\n",
       "      <td>sangat rekomended buat kalian yang mau melatih...</td>\n",
       "      <td>sangat rekomended buat kalian yang mau melatih...</td>\n",
       "      <td>[sangat, rekomended, buat, kalian, yang, mau, ...</td>\n",
       "      <td>[rekomended, melatih, kesabaran, setipis, tisu...</td>\n",
       "      <td>rekomended melatih kesabaran setipis tisu diba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>98f0f6db-3225-4f54-94cc-0d920622db63</td>\n",
       "      <td>Pengguna Google</td>\n",
       "      <td>https://play-lh.googleusercontent.com/EGemoI2N...</td>\n",
       "      <td>Untuk sekarang saya kasih segini dulu. Kecewa ...</td>\n",
       "      <td>1</td>\n",
       "      <td>96</td>\n",
       "      <td>1.59.0</td>\n",
       "      <td>2024-12-19 20:01:50</td>\n",
       "      <td>Hi, Kak. Maaf banget jadi bikin gak nyaman. Ni...</td>\n",
       "      <td>2024-12-20 07:08:21</td>\n",
       "      <td>1.59.0</td>\n",
       "      <td>Untuk sekarang saya kasih segini dulu Kecewa p...</td>\n",
       "      <td>untuk sekarang saya kasih segini dulu kecewa p...</td>\n",
       "      <td>untuk sekarang saya kasih segini dulu kecewa p...</td>\n",
       "      <td>[untuk, sekarang, saya, kasih, segini, dulu, k...</td>\n",
       "      <td>[kasih, segini, kecewa, parah, udah, makai, by...</td>\n",
       "      <td>kasih segini kecewa parah udah makai byu sinya...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>427de1b2-99ea-4d93-86cb-acb059c1ef7f</td>\n",
       "      <td>Pengguna Google</td>\n",
       "      <td>https://play-lh.googleusercontent.com/EGemoI2N...</td>\n",
       "      <td>Sayang ya. By.u ini menarik sekali tapi bener ...</td>\n",
       "      <td>3</td>\n",
       "      <td>33</td>\n",
       "      <td>1.59.0</td>\n",
       "      <td>2024-12-26 16:10:50</td>\n",
       "      <td>Hi, Kak. Maaf banget jadi bikin gak nyaman. Ni...</td>\n",
       "      <td>2024-12-26 21:11:56</td>\n",
       "      <td>1.59.0</td>\n",
       "      <td>Sayang ya Byu ini menarik sekali tapi bener ka...</td>\n",
       "      <td>sayang ya byu ini menarik sekali tapi bener ka...</td>\n",
       "      <td>sayang ya byu ini menarik sekali tapi bener ka...</td>\n",
       "      <td>[sayang, ya, byu, ini, menarik, sekali, tapi, ...</td>\n",
       "      <td>[sayang, byu, menarik, bener, orang², sinyal, ...</td>\n",
       "      <td>sayang byu menarik bener orang² sinyal maju mu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9e145aa3-7c51-402d-b8da-8911061e07da</td>\n",
       "      <td>Pengguna Google</td>\n",
       "      <td>https://play-lh.googleusercontent.com/EGemoI2N...</td>\n",
       "      <td>Cuaca baik baiknya saja dan paket data baru sa...</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1.59.0</td>\n",
       "      <td>2025-01-05 17:22:14</td>\n",
       "      <td>Hi, Kak. Maaf banget jadi bikin gak nyaman. Ni...</td>\n",
       "      <td>2025-01-06 08:36:34</td>\n",
       "      <td>1.59.0</td>\n",
       "      <td>Cuaca baik baiknya saja dan paket data baru sa...</td>\n",
       "      <td>cuaca baik baiknya saja dan paket data baru sa...</td>\n",
       "      <td>cuaca baik baiknya saja dan paket data baru sa...</td>\n",
       "      <td>[cuaca, baik, baiknya, saja, dan, paket, data,...</td>\n",
       "      <td>[cuaca, baiknya, paket, data, dibeli, diaktifk...</td>\n",
       "      <td>cuaca baiknya paket data dibeli diaktifkan jar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b9a8d064-7751-436c-bb53-35fa167a15bb</td>\n",
       "      <td>Pengguna Google</td>\n",
       "      <td>https://play-lh.googleusercontent.com/EGemoI2N...</td>\n",
       "      <td>sy pengguna provider ini dari awal 2020, sedih...</td>\n",
       "      <td>1</td>\n",
       "      <td>49</td>\n",
       "      <td>1.59.0</td>\n",
       "      <td>2025-01-02 02:38:06</td>\n",
       "      <td>Hi, Kak. Maaf banget jadi bikin gak nyaman. Ni...</td>\n",
       "      <td>2025-01-02 07:08:01</td>\n",
       "      <td>1.59.0</td>\n",
       "      <td>sy pengguna provider ini dari awal  sedih bang...</td>\n",
       "      <td>sy pengguna provider ini dari awal  sedih bang...</td>\n",
       "      <td>sy pengguna provider ini dari awal sedih bange...</td>\n",
       "      <td>[sy, pengguna, provider, ini, dari, awal, sedi...</td>\n",
       "      <td>[sy, pengguna, provider, sedih, banget, perfor...</td>\n",
       "      <td>sy pengguna provider sedih banget performa byu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116995</th>\n",
       "      <td>f6d11492-a8e3-4318-8d82-e324925da144</td>\n",
       "      <td>Pengguna Google</td>\n",
       "      <td>https://play-lh.googleusercontent.com/EGemoI2N...</td>\n",
       "      <td>mantap</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>787</td>\n",
       "      <td>2022-04-02 15:52:54</td>\n",
       "      <td>Hai Kak! Makasih banyak ya buat feedbacknya. N...</td>\n",
       "      <td>2022-04-02 16:08:15</td>\n",
       "      <td>787</td>\n",
       "      <td>mantap</td>\n",
       "      <td>mantap</td>\n",
       "      <td>mantap</td>\n",
       "      <td>[mantap]</td>\n",
       "      <td>[mantap]</td>\n",
       "      <td>mantap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116996</th>\n",
       "      <td>aebad197-cc62-4849-a4f3-819498a2da7f</td>\n",
       "      <td>Pengguna Google</td>\n",
       "      <td>https://play-lh.googleusercontent.com/EGemoI2N...</td>\n",
       "      <td>bagus</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>787</td>\n",
       "      <td>2022-04-02 12:50:54</td>\n",
       "      <td>Hai Kak Damar! Makasih ya buat feedbacknya. Ni...</td>\n",
       "      <td>2022-04-02 13:04:33</td>\n",
       "      <td>787</td>\n",
       "      <td>bagus</td>\n",
       "      <td>bagus</td>\n",
       "      <td>bagus</td>\n",
       "      <td>[bagus]</td>\n",
       "      <td>[bagus]</td>\n",
       "      <td>bagus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116997</th>\n",
       "      <td>ced86d00-73f2-4282-8053-c42e70cd7960</td>\n",
       "      <td>Pengguna Google</td>\n",
       "      <td>https://play-lh.googleusercontent.com/EGemoI2N...</td>\n",
       "      <td>mantaappp</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>787</td>\n",
       "      <td>2022-03-21 02:50:01</td>\n",
       "      <td>Hai Kak! Makasih banget yaa buat feedbacknya. ...</td>\n",
       "      <td>2022-03-21 02:56:35</td>\n",
       "      <td>787</td>\n",
       "      <td>mantaappp</td>\n",
       "      <td>mantaappp</td>\n",
       "      <td>mantaappp</td>\n",
       "      <td>[mantaappp]</td>\n",
       "      <td>[mantaappp]</td>\n",
       "      <td>mantaappp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116998</th>\n",
       "      <td>ddc28e7d-8b22-458e-a4a2-1eef2e22af2e</td>\n",
       "      <td>Pengguna Google</td>\n",
       "      <td>https://play-lh.googleusercontent.com/EGemoI2N...</td>\n",
       "      <td>mantap</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>787</td>\n",
       "      <td>2022-03-26 14:28:29</td>\n",
       "      <td>Hai Kak. Makasih banyak buat feedbacknya ya. N...</td>\n",
       "      <td>2022-03-26 14:33:24</td>\n",
       "      <td>787</td>\n",
       "      <td>mantap</td>\n",
       "      <td>mantap</td>\n",
       "      <td>mantap</td>\n",
       "      <td>[mantap]</td>\n",
       "      <td>[mantap]</td>\n",
       "      <td>mantap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116999</th>\n",
       "      <td>7dd50dd0-f83d-4743-9edd-834eee45b1e8</td>\n",
       "      <td>Pengguna Google</td>\n",
       "      <td>https://play-lh.googleusercontent.com/EGemoI2N...</td>\n",
       "      <td>mantap</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>787</td>\n",
       "      <td>2022-03-27 07:16:24</td>\n",
       "      <td>Hai Kak.  Makasih buat feedbacknya ya Kak. Nin...</td>\n",
       "      <td>2022-03-27 07:21:48</td>\n",
       "      <td>787</td>\n",
       "      <td>mantap</td>\n",
       "      <td>mantap</td>\n",
       "      <td>mantap</td>\n",
       "      <td>[mantap]</td>\n",
       "      <td>[mantap]</td>\n",
       "      <td>mantap</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>91268 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    reviewId         userName  \\\n",
       "0       77b1715e-885d-47c7-b958-41c18f60593d  Pengguna Google   \n",
       "1       98f0f6db-3225-4f54-94cc-0d920622db63  Pengguna Google   \n",
       "2       427de1b2-99ea-4d93-86cb-acb059c1ef7f  Pengguna Google   \n",
       "3       9e145aa3-7c51-402d-b8da-8911061e07da  Pengguna Google   \n",
       "4       b9a8d064-7751-436c-bb53-35fa167a15bb  Pengguna Google   \n",
       "...                                      ...              ...   \n",
       "116995  f6d11492-a8e3-4318-8d82-e324925da144  Pengguna Google   \n",
       "116996  aebad197-cc62-4849-a4f3-819498a2da7f  Pengguna Google   \n",
       "116997  ced86d00-73f2-4282-8053-c42e70cd7960  Pengguna Google   \n",
       "116998  ddc28e7d-8b22-458e-a4a2-1eef2e22af2e  Pengguna Google   \n",
       "116999  7dd50dd0-f83d-4743-9edd-834eee45b1e8  Pengguna Google   \n",
       "\n",
       "                                                userImage  \\\n",
       "0       https://play-lh.googleusercontent.com/EGemoI2N...   \n",
       "1       https://play-lh.googleusercontent.com/EGemoI2N...   \n",
       "2       https://play-lh.googleusercontent.com/EGemoI2N...   \n",
       "3       https://play-lh.googleusercontent.com/EGemoI2N...   \n",
       "4       https://play-lh.googleusercontent.com/EGemoI2N...   \n",
       "...                                                   ...   \n",
       "116995  https://play-lh.googleusercontent.com/EGemoI2N...   \n",
       "116996  https://play-lh.googleusercontent.com/EGemoI2N...   \n",
       "116997  https://play-lh.googleusercontent.com/EGemoI2N...   \n",
       "116998  https://play-lh.googleusercontent.com/EGemoI2N...   \n",
       "116999  https://play-lh.googleusercontent.com/EGemoI2N...   \n",
       "\n",
       "                                                  content  score  \\\n",
       "0       sangat rekomended buat kalian yang mau melatih...      1   \n",
       "1       Untuk sekarang saya kasih segini dulu. Kecewa ...      1   \n",
       "2       Sayang ya. By.u ini menarik sekali tapi bener ...      3   \n",
       "3       Cuaca baik baiknya saja dan paket data baru sa...      3   \n",
       "4       sy pengguna provider ini dari awal 2020, sedih...      1   \n",
       "...                                                   ...    ...   \n",
       "116995                                             mantap      5   \n",
       "116996                                              bagus      5   \n",
       "116997                                          mantaappp      5   \n",
       "116998                                             mantap      5   \n",
       "116999                                             mantap      5   \n",
       "\n",
       "        thumbsUpCount reviewCreatedVersion                  at  \\\n",
       "0                 361               1.59.0 2024-12-19 23:08:33   \n",
       "1                  96               1.59.0 2024-12-19 20:01:50   \n",
       "2                  33               1.59.0 2024-12-26 16:10:50   \n",
       "3                   7               1.59.0 2025-01-05 17:22:14   \n",
       "4                  49               1.59.0 2025-01-02 02:38:06   \n",
       "...               ...                  ...                 ...   \n",
       "116995              0                  787 2022-04-02 15:52:54   \n",
       "116996              0                  787 2022-04-02 12:50:54   \n",
       "116997              0                  787 2022-03-21 02:50:01   \n",
       "116998              0                  787 2022-03-26 14:28:29   \n",
       "116999              0                  787 2022-03-27 07:16:24   \n",
       "\n",
       "                                             replyContent           repliedAt  \\\n",
       "0       Hi, Kak. Maaf banget jadi bikin gak nyaman. Ni... 2024-12-20 07:05:11   \n",
       "1       Hi, Kak. Maaf banget jadi bikin gak nyaman. Ni... 2024-12-20 07:08:21   \n",
       "2       Hi, Kak. Maaf banget jadi bikin gak nyaman. Ni... 2024-12-26 21:11:56   \n",
       "3       Hi, Kak. Maaf banget jadi bikin gak nyaman. Ni... 2025-01-06 08:36:34   \n",
       "4       Hi, Kak. Maaf banget jadi bikin gak nyaman. Ni... 2025-01-02 07:08:01   \n",
       "...                                                   ...                 ...   \n",
       "116995  Hai Kak! Makasih banyak ya buat feedbacknya. N... 2022-04-02 16:08:15   \n",
       "116996  Hai Kak Damar! Makasih ya buat feedbacknya. Ni... 2022-04-02 13:04:33   \n",
       "116997  Hai Kak! Makasih banget yaa buat feedbacknya. ... 2022-03-21 02:56:35   \n",
       "116998  Hai Kak. Makasih banyak buat feedbacknya ya. N... 2022-03-26 14:33:24   \n",
       "116999  Hai Kak.  Makasih buat feedbacknya ya Kak. Nin... 2022-03-27 07:21:48   \n",
       "\n",
       "       appVersion                                         text_clean  \\\n",
       "0          1.59.0  sangat rekomended buat kalian yang mau melatih...   \n",
       "1          1.59.0  Untuk sekarang saya kasih segini dulu Kecewa p...   \n",
       "2          1.59.0  Sayang ya Byu ini menarik sekali tapi bener ka...   \n",
       "3          1.59.0  Cuaca baik baiknya saja dan paket data baru sa...   \n",
       "4          1.59.0  sy pengguna provider ini dari awal  sedih bang...   \n",
       "...           ...                                                ...   \n",
       "116995        787                                             mantap   \n",
       "116996        787                                              bagus   \n",
       "116997        787                                          mantaappp   \n",
       "116998        787                                             mantap   \n",
       "116999        787                                             mantap   \n",
       "\n",
       "                                     text_casefoldingText  \\\n",
       "0       sangat rekomended buat kalian yang mau melatih...   \n",
       "1       untuk sekarang saya kasih segini dulu kecewa p...   \n",
       "2       sayang ya byu ini menarik sekali tapi bener ka...   \n",
       "3       cuaca baik baiknya saja dan paket data baru sa...   \n",
       "4       sy pengguna provider ini dari awal  sedih bang...   \n",
       "...                                                   ...   \n",
       "116995                                             mantap   \n",
       "116996                                              bagus   \n",
       "116997                                          mantaappp   \n",
       "116998                                             mantap   \n",
       "116999                                             mantap   \n",
       "\n",
       "                                          text_slangwords  \\\n",
       "0       sangat rekomended buat kalian yang mau melatih...   \n",
       "1       untuk sekarang saya kasih segini dulu kecewa p...   \n",
       "2       sayang ya byu ini menarik sekali tapi bener ka...   \n",
       "3       cuaca baik baiknya saja dan paket data baru sa...   \n",
       "4       sy pengguna provider ini dari awal sedih bange...   \n",
       "...                                                   ...   \n",
       "116995                                             mantap   \n",
       "116996                                              bagus   \n",
       "116997                                          mantaappp   \n",
       "116998                                             mantap   \n",
       "116999                                             mantap   \n",
       "\n",
       "                                      text_tokenizingText  \\\n",
       "0       [sangat, rekomended, buat, kalian, yang, mau, ...   \n",
       "1       [untuk, sekarang, saya, kasih, segini, dulu, k...   \n",
       "2       [sayang, ya, byu, ini, menarik, sekali, tapi, ...   \n",
       "3       [cuaca, baik, baiknya, saja, dan, paket, data,...   \n",
       "4       [sy, pengguna, provider, ini, dari, awal, sedi...   \n",
       "...                                                   ...   \n",
       "116995                                           [mantap]   \n",
       "116996                                            [bagus]   \n",
       "116997                                        [mantaappp]   \n",
       "116998                                           [mantap]   \n",
       "116999                                           [mantap]   \n",
       "\n",
       "                                            text_stopword  \\\n",
       "0       [rekomended, melatih, kesabaran, setipis, tisu...   \n",
       "1       [kasih, segini, kecewa, parah, udah, makai, by...   \n",
       "2       [sayang, byu, menarik, bener, orang², sinyal, ...   \n",
       "3       [cuaca, baiknya, paket, data, dibeli, diaktifk...   \n",
       "4       [sy, pengguna, provider, sedih, banget, perfor...   \n",
       "...                                                   ...   \n",
       "116995                                           [mantap]   \n",
       "116996                                            [bagus]   \n",
       "116997                                        [mantaappp]   \n",
       "116998                                           [mantap]   \n",
       "116999                                           [mantap]   \n",
       "\n",
       "                                               text_akhir  \n",
       "0       rekomended melatih kesabaran setipis tisu diba...  \n",
       "1       kasih segini kecewa parah udah makai byu sinya...  \n",
       "2       sayang byu menarik bener orang² sinyal maju mu...  \n",
       "3       cuaca baiknya paket data dibeli diaktifkan jar...  \n",
       "4       sy pengguna provider sedih banget performa byu...  \n",
       "...                                                   ...  \n",
       "116995                                             mantap  \n",
       "116996                                              bagus  \n",
       "116997                                          mantaappp  \n",
       "116998                                             mantap  \n",
       "116999                                             mantap  \n",
       "\n",
       "[91268 rows x 17 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import requests\n",
    "from io import StringIO\n",
    " \n",
    "# Membaca data kamus kata-kata positif dari GitHub\n",
    "lexicon_positive = dict()\n",
    " \n",
    "response = requests.get('https://raw.githubusercontent.com/angelmetanosaa/dataset/main/lexicon_positive.csv')\n",
    "# Mengirim permintaan HTTP untuk mendapatkan file CSV dari GitHub\n",
    " \n",
    "if response.status_code == 200:\n",
    "    # Jika permintaan berhasil\n",
    "    reader = csv.reader(StringIO(response.text), delimiter=',')\n",
    "    # Membaca teks respons sebagai file CSV menggunakan pembaca CSV dengan pemisah koma\n",
    " \n",
    "    for row in reader:\n",
    "        # Mengulangi setiap baris dalam file CSV\n",
    "        lexicon_positive[row[0]] = int(row[1])\n",
    "        # Menambahkan kata-kata positif dan skornya ke dalam kamus lexicon_positive\n",
    "else:\n",
    "    print(\"Failed to fetch positive lexicon data\")\n",
    " \n",
    "# Membaca data kamus kata-kata negatif dari GitHub\n",
    "lexicon_negative = dict()\n",
    " \n",
    "response = requests.get('https://raw.githubusercontent.com/angelmetanosaa/dataset/main/lexicon_negative.csv')\n",
    "# Mengirim permintaan HTTP untuk mendapatkan file CSV dari GitHub\n",
    " \n",
    "if response.status_code == 200:\n",
    "    # Jika permintaan berhasil\n",
    "    reader = csv.reader(StringIO(response.text), delimiter=',')\n",
    "    # Membaca teks respons sebagai file CSV menggunakan pembaca CSV dengan pemisah koma\n",
    " \n",
    "    for row in reader:\n",
    "        # Mengulangi setiap baris dalam file CSV\n",
    "        lexicon_negative[row[0]] = int(row[1])\n",
    "        # Menambahkan kata-kata negatif dan skornya dalam kamus lexicon_negative\n",
    "else:\n",
    "    print(\"Failed to fetch negative lexicon data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi untuk menentukan polaritas sentimen dari tweet\n",
    " \n",
    "def sentiment_analysis_lexicon_indonesia(text):\n",
    "    #for word in text:\n",
    " \n",
    "    score = 0\n",
    "    # Inisialisasi skor sentimen ke 0\n",
    " \n",
    "    for word in text:\n",
    "        # Mengulangi setiap kata dalam teks\n",
    " \n",
    "        if (word in lexicon_positive):\n",
    "            score = score + lexicon_positive[word]\n",
    "            # Jika kata ada dalam kamus positif, tambahkan skornya ke skor sentimen\n",
    " \n",
    "    for word in text:\n",
    "        # Mengulangi setiap kata dalam teks (sekali lagi)\n",
    " \n",
    "        if (word in lexicon_negative):\n",
    "            score = score + lexicon_negative[word]\n",
    "            # Jika kata ada dalam kamus negatif, kurangkan skornya dari skor sentimen\n",
    " \n",
    "    polarity=''\n",
    "    # Inisialisasi variabel polaritas\n",
    " \n",
    "    if (score >= 0):\n",
    "        polarity = 'positive'\n",
    "        # Jika skor sentimen lebih besar atau sama dengan 0, maka polaritas adalah positif\n",
    "    elif (score < 0):\n",
    "        polarity = 'negative'\n",
    "        # Jika skor sentimen kurang dari 0, maka polaritas adalah negatif\n",
    " \n",
    "    # else:\n",
    "    #     polarity = 'neutral'\n",
    "    # Ini adalah bagian yang bisa digunakan untuk menentukan polaritas netral jika diperlukan\n",
    " \n",
    "    return score, polarity\n",
    "    # Mengembalikan skor sentimen dan polaritas teks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "polarity\n",
      "positive    47476\n",
      "negative    43792\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "results = clean_df['text_stopword'].apply(sentiment_analysis_lexicon_indonesia)\n",
    "results = list(zip(*results))\n",
    "clean_df['polarity_score'] = results[0]\n",
    "clean_df['polarity'] = results[1]\n",
    "print(clean_df['polarity'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 14\u001b[0m\n\u001b[0;32m      6\u001b[0m custom_cmap \u001b[38;5;241m=\u001b[39m ListedColormap([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m#ff9999\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m#66b3ff\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m#99ff99\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m#ffcc99\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Membuat word cloud dengan kustomisasi\u001b[39;00m\n\u001b[0;32m      9\u001b[0m wordcloud \u001b[38;5;241m=\u001b[39m \u001b[43mWordCloud\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwidth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m800\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m400\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbackground_color\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mblack\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolormap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_cmap\u001b[49m\n\u001b[1;32m---> 14\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\IntermediateDS\\venv\\Lib\\site-packages\\wordcloud\\wordcloud.py:642\u001b[0m, in \u001b[0;36mWordCloud.generate\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mgenerate\u001b[39m(\u001b[38;5;28mself\u001b[39m, text):\n\u001b[0;32m    628\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Generate wordcloud from text.\u001b[39;00m\n\u001b[0;32m    629\u001b[0m \n\u001b[0;32m    630\u001b[0m \u001b[38;5;124;03m    The input \"text\" is expected to be a natural text. If you pass a sorted\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    640\u001b[0m \u001b[38;5;124;03m    self\u001b[39;00m\n\u001b[0;32m    641\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 642\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_from_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\IntermediateDS\\venv\\Lib\\site-packages\\wordcloud\\wordcloud.py:624\u001b[0m, in \u001b[0;36mWordCloud.generate_from_text\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    607\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Generate wordcloud from text.\u001b[39;00m\n\u001b[0;32m    608\u001b[0m \n\u001b[0;32m    609\u001b[0m \u001b[38;5;124;03mThe input \"text\" is expected to be a natural text. If you pass a sorted\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    621\u001b[0m \u001b[38;5;124;03mself\u001b[39;00m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    623\u001b[0m words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_text(text)\n\u001b[1;32m--> 624\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_from_frequencies\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwords\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\IntermediateDS\\venv\\Lib\\site-packages\\wordcloud\\wordcloud.py:535\u001b[0m, in \u001b[0;36mWordCloud.generate_from_frequencies\u001b[1;34m(self, frequencies, max_font_size)\u001b[0m\n\u001b[0;32m    533\u001b[0m x, y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(result) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmargin \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m    534\u001b[0m \u001b[38;5;66;03m# actually draw the text\u001b[39;00m\n\u001b[1;32m--> 535\u001b[0m \u001b[43mdraw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mword\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwhite\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfont\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransposed_font\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    536\u001b[0m positions\u001b[38;5;241m.\u001b[39mappend((x, y))\n\u001b[0;32m    537\u001b[0m orientations\u001b[38;5;241m.\u001b[39mappend(orientation)\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\IntermediateDS\\venv\\Lib\\site-packages\\PIL\\ImageDraw.py:698\u001b[0m, in \u001b[0;36mImageDraw.text\u001b[1;34m(self, xy, text, fill, font, anchor, spacing, align, direction, features, language, stroke_width, stroke_fill, embedded_color, *args, **kwargs)\u001b[0m\n\u001b[0;32m    695\u001b[0m     draw_text(ink, \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    696\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    697\u001b[0m     \u001b[38;5;66;03m# Only draw normal text\u001b[39;00m\n\u001b[1;32m--> 698\u001b[0m     \u001b[43mdraw_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mink\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\IntermediateDS\\venv\\Lib\\site-packages\\PIL\\ImageDraw.py:682\u001b[0m, in \u001b[0;36mImageDraw.text.<locals>.draw_text\u001b[1;34m(ink, stroke_width)\u001b[0m\n\u001b[0;32m    678\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mim\u001b[38;5;241m.\u001b[39mpaste(\n\u001b[0;32m    679\u001b[0m             color, (x, y, x \u001b[38;5;241m+\u001b[39m mask\u001b[38;5;241m.\u001b[39msize[\u001b[38;5;241m0\u001b[39m], y \u001b[38;5;241m+\u001b[39m mask\u001b[38;5;241m.\u001b[39msize[\u001b[38;5;241m1\u001b[39m]), mask\n\u001b[0;32m    680\u001b[0m         )\n\u001b[0;32m    681\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 682\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw_bitmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcoord\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mink\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "wc_general = clean_df['polarity']\n",
    "\n",
    "for word in wc_general:\n",
    "    # Custom colormap\n",
    "    custom_cmap = ListedColormap(['#ff9999', '#66b3ff', '#99ff99', '#ffcc99'])\n",
    "\n",
    "    # Membuat word cloud dengan kustomisasi\n",
    "    wordcloud = WordCloud(\n",
    "        width=800,\n",
    "        height=400,\n",
    "        background_color='black',\n",
    "        colormap=custom_cmap\n",
    "    ).generate(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title('Hasil WordCloud Polarity General')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
